{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21e57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/n23w_6ys7gq5ng7vbsvdsz500000gn/T/ipykernel_91489/1419924216.py:18: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "Fetching intros: 100%|██████████| 1/1 [00:00<00:00,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before dedupe: 1, after dedupe: 1 (unique symbols)\n",
      "✅ Exported 1 rows to sp500_wiki_intros_full.csv\n",
      "Unique symbols: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import csv\n",
    "\n",
    "N = 1 # How many companies to process; set to a higher number for full run\n",
    "\n",
    "# Step 1: Get S&P 500 companies table\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; sp500-fetcher/2.0; +https://example.com)\"}\n",
    "resp = requests.get(url, headers=headers, timeout=20)\n",
    "resp.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "df = pd.read_html(str(table))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69945370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Get company Wikipedia URLs\n",
    "base = \"https://en.wikipedia.org\"\n",
    "links = [base + a[\"href\"] for a in table.select(\"tbody tr td:nth-of-type(2) a[href]\")]\n",
    "links = links[:N]              # trim links list first\n",
    "df = df.iloc[:N].copy()        # then trim df to same length\n",
    "df[\"Wikipedia_URL\"] = links    # now they match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Fetch the *intro paragraphs* directly from the article HTML\n",
    "def get_intro(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        # Target main content area\n",
    "        content_div = soup.find(\"div\", {\"class\": \"mw-parser-output\"})\n",
    "        if not content_div:\n",
    "            return \"\"\n",
    "\n",
    "        # Gather all <p> tags before the first <h2> (usually the intro)\n",
    "        paragraphs = []\n",
    "        for el in content_div.find_all([\"p\", \"h2\"], recursive=False):\n",
    "            if el.name == \"h2\":\n",
    "                break\n",
    "            if el.name == \"p\" and el.get_text(strip=True):\n",
    "                paragraphs.append(el.get_text(\" \", strip=True))\n",
    "\n",
    "        intro_text = \" \".join(paragraphs)\n",
    "        return intro_text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f8b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Fetch intros for all companies\n",
    "intros = []\n",
    "for link in tqdm(df[\"Wikipedia_URL\"], desc=\"Fetching intros\"):\n",
    "    intros.append(get_intro(link))\n",
    "    time.sleep(0.3)  # polite delay\n",
    "\n",
    "df[\"Intro\"] = intros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Clean and deduplicate data\n",
    "df = df[[\"Symbol\", \"Security\", \"Wikipedia_URL\", \"Intro\"]].copy()\n",
    "df[\"Symbol\"] = df[\"Symbol\"].astype(str).str.strip()\n",
    "df[\"Security\"] = df[\"Security\"].astype(str).str.strip()\n",
    "df = df[df[\"Symbol\"] != \"\"]\n",
    "df = df.dropna(subset=[\"Symbol\"])\n",
    "\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=[\"Symbol\"], keep=\"first\").reset_index(drop=True)\n",
    "after = len(df)\n",
    "\n",
    "print(f\"Rows before dedupe: {before}, after dedupe: {after} (unique symbols)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8662fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Add ID and save to CSV\n",
    "df.insert(0, \"ID\", range(1, len(df) + 1))\n",
    "out_path = \"sp500_wiki_intros_full.csv\"\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL)\n",
    "\n",
    "print(f\"✅ Exported {len(df)} rows to {out_path}\")\n",
    "print(f\"Unique symbols: {df['Symbol'].nunique()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
