{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa21e57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ [Step 1] Starting: Fetching S&P 500 companies table from Wikipedia...\n",
      "‚úÖ Wikipedia page fetched successfully.\n",
      "‚úÖ [Step 1 Complete] Retrieved S&P 500 table with 503 entries in 0.37s.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/n23w_6ys7gq5ng7vbsvdsz500000gn/T/ipykernel_91800/2082349891.py:25: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "N = 1  # How many companies to process; set higher for full run\n",
    "\n",
    "print(\"üü¢ [Step 1] Starting: Fetching S&P 500 companies table from Wikipedia...\")\n",
    "start_time = time.time()\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; sp500-fetcher/2.0; +https://example.com)\"}\n",
    "\n",
    "try:\n",
    "    resp = requests.get(url, headers=headers, timeout=20)\n",
    "    resp.raise_for_status()\n",
    "    print(\"‚úÖ Wikipedia page fetched successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error fetching page: {e}\")\n",
    "    raise\n",
    "\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "df = pd.read_html(str(table))[0]\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"‚úÖ [Step 1 Complete] Retrieved S&P 500 table with {len(df)} entries in {elapsed:.2f}s.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69945370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ [Step 2] Starting: Extracting company Wikipedia URLs...\n",
      "‚úÖ [Step 2 Complete] Collected 1 Wikipedia URLs in 0.05s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üü¢ [Step 2] Starting: Extracting company Wikipedia URLs...\")\n",
    "step2_start = time.time()\n",
    "\n",
    "base = \"https://en.wikipedia.org\"\n",
    "\n",
    "# Extract links from the S&P 500 table\n",
    "links = [base + a[\"href\"] for a in table.select(\"tbody tr td:nth-of-type(2) a[href]\")]\n",
    "\n",
    "# Limit to first N companies\n",
    "links = links[:N]\n",
    "df = df.iloc[:N].copy()\n",
    "df[\"Wikipedia_URL\"] = links\n",
    "\n",
    "elapsed = time.time() - step2_start\n",
    "print(f\"‚úÖ [Step 2 Complete] Collected {len(links)} Wikipedia URLs in {elapsed:.2f}s.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc2401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ [Step 3] Starting: Defining function to fetch Wikipedia intro paragraphs...\n",
      "‚úÖ [Step 3 Complete] Intro extraction function defined successfully in 0.00s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üü¢ [Step 3] Starting: Defining function to fetch Wikipedia intro paragraphs...\")\n",
    "step3_start = time.time()\n",
    "\n",
    "# Step 3: Fetch the *intro paragraphs* directly from the article HTML\n",
    "def get_intro(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        # Target main content area\n",
    "        content_div = soup.find(\"div\", {\"class\": \"mw-parser-output\"})\n",
    "        if not content_div:\n",
    "            return \"\"\n",
    "\n",
    "        # Gather all <p> tags before the first <h2> (usually the intro)\n",
    "        paragraphs = []\n",
    "        for el in content_div.find_all([\"p\", \"h2\"], recursive=False):\n",
    "            if el.name == \"h2\":\n",
    "                break\n",
    "            if el.name == \"p\" and el.get_text(strip=True):\n",
    "                paragraphs.append(el.get_text(\" \", strip=True))\n",
    "\n",
    "        intro_text = \" \".join(paragraphs)\n",
    "        return intro_text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"\"\n",
    "\n",
    "elapsed = time.time() - step3_start\n",
    "print(f\"‚úÖ [Step 3 Complete] Intro extraction function defined successfully in {elapsed:.2f}s.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "597f8b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ [Step 4] Starting: Fetching intros for all companies...\n",
      "   ‚è±Ô∏è  Fetched intros for 1/1 companies...\n",
      "‚úÖ [Step 4 Complete] All intros fetched successfully in 0.53s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üü¢ [Step 4] Starting: Fetching intros for all companies...\")\n",
    "step4_start = time.time()\n",
    "\n",
    "# Step 4: Fetch intros for all companies\n",
    "intros = []\n",
    "for idx, link in enumerate(df[\"Wikipedia_URL\"], 1):\n",
    "    intros.append(get_intro(link))\n",
    "    time.sleep(0.3)  # polite delay\n",
    "\n",
    "    # Print status every 10 companies\n",
    "    if idx % 10 == 0 or idx == len(df):\n",
    "        print(f\"   ‚è±Ô∏è  Fetched intros for {idx}/{len(df)} companies...\")\n",
    "\n",
    "df[\"Intro\"] = intros\n",
    "\n",
    "elapsed = time.time() - step4_start\n",
    "print(f\"‚úÖ [Step 4 Complete] All intros fetched successfully in {elapsed:.2f}s.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b00b403e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ [Step 5] Starting: Cleaning and deduplicating data...\n",
      "‚úÖ [Step 5 Complete] Cleaned and deduplicated data in 0.01s. Rows before: 1, after: 1 (unique symbols)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üü¢ [Step 5] Starting: Cleaning and deduplicating data...\")\n",
    "step5_start = time.time()\n",
    "\n",
    "# Step 5: Clean and deduplicate data\n",
    "df = df[[\"Symbol\", \"Security\", \"Wikipedia_URL\", \"Intro\"]].copy()\n",
    "df[\"Symbol\"] = df[\"Symbol\"].astype(str).str.strip()\n",
    "df[\"Security\"] = df[\"Security\"].astype(str).str.strip()\n",
    "df = df[df[\"Symbol\"] != \"\"]\n",
    "df = df.dropna(subset=[\"Symbol\"])\n",
    "\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=[\"Symbol\"], keep=\"first\").reset_index(drop=True)\n",
    "after = len(df)\n",
    "\n",
    "elapsed = time.time() - step5_start\n",
    "print(f\"‚úÖ [Step 5 Complete] Cleaned and deduplicated data in {elapsed:.2f}s. Rows before: {before}, after: {after} (unique symbols)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8662fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ [Step 6] Starting: Adding ID and saving to CSV...\n",
      "‚úÖ [Step 6 Complete] Exported 1 rows to sp500_wiki_intros_full.csv in 0.01s.\n",
      "   üîπ Unique symbols: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üü¢ [Step 6] Starting: Adding ID and saving to CSV...\")\n",
    "step6_start = time.time()\n",
    "\n",
    "# Step 6: Add ID and save to CSV\n",
    "df.insert(0, \"ID\", range(1, len(df) + 1))\n",
    "out_path = \"sp500_wiki_intros_full.csv\"\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL)\n",
    "\n",
    "elapsed = time.time() - step6_start\n",
    "print(f\"‚úÖ [Step 6 Complete] Exported {len(df)} rows to {out_path} in {elapsed:.2f}s.\")\n",
    "print(f\"   üîπ Unique symbols: {df['Symbol'].nunique()}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
