{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f5deb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "585ec123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacks\\AppData\\Local\\Temp\\ipykernel_6792\\3343189255.py:16: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "Fetching intros: 100%|██████████| 503/503 [03:32<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported 503 rows to sp500_wiki_intros.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Step 1: Get S&P 500 companies table\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; sp500-fetcher/1.0; +https://example.com)\"}\n",
    "resp = requests.get(url, headers=headers, timeout=20)\n",
    "resp.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "\n",
    "df = pd.read_html(str(table))[0]\n",
    "\n",
    "# Step 2: Get company Wikipedia URLs\n",
    "base = \"https://en.wikipedia.org\"\n",
    "links = [base + a[\"href\"] for a in table.select(\"tbody tr td:nth-of-type(2) a[href]\")]\n",
    "df = df.iloc[:len(links)].copy()\n",
    "df[\"Wikipedia_URL\"] = links\n",
    "\n",
    "# Step 3: Fetch short intro for each company\n",
    "def get_intro(url):\n",
    "    title = url.split(\"/wiki/\")[-1]\n",
    "    api_url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{title}\"\n",
    "    try:\n",
    "        r = requests.get(api_url, headers=headers, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        return r.json().get(\"extract\", \"\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "intros = []\n",
    "for link in tqdm(df[\"Wikipedia_URL\"], desc=\"Fetching intros\"):\n",
    "    intros.append(get_intro(link))\n",
    "    time.sleep(0.3)  # delay to avoid being blocked\n",
    "\n",
    "# Step 4: Keep only required columns and add ID\n",
    "df = df[[\"Symbol\", \"Security\", \"Wikipedia_URL\"]].copy()\n",
    "df[\"Intro\"] = intros\n",
    "df.insert(0, \"ID\", range(1, len(df) + 1))\n",
    "\n",
    "# Step 5: Save to CSV\n",
    "df.to_csv(\"sp500_wiki_intros.csv\", index=False)\n",
    "print(\"✅ Exported\", len(df), \"rows to sp500_wiki_intros.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d3ee022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before dedupe: 503, after dedupe: 503 (unique symbols)\n",
      "✅ Cleaned and saved 503 rows to sp500_wiki_intros_clean.csv\n",
      "Unique symbols: 503\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Drop existing ID column if it already exists\n",
    "if \"ID\" in df.columns:\n",
    "    df = df.drop(columns=[\"ID\"])\n",
    "\n",
    "# 1) Basic cleaning\n",
    "df[\"Symbol\"] = df[\"Symbol\"].astype(str).str.strip()\n",
    "df[\"Security\"] = df[\"Security\"].astype(str).str.strip()\n",
    "df = df[df[\"Symbol\"] != \"\"]\n",
    "df = df.dropna(subset=[\"Symbol\"])\n",
    "\n",
    "# 2) Deduplicate by ticker symbol\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=[\"Symbol\"], keep=\"first\").reset_index(drop=True)\n",
    "after = len(df)\n",
    "\n",
    "print(f\"Rows before dedupe: {before}, after dedupe: {after} (unique symbols)\")\n",
    "\n",
    "# 3) Recreate ID starting at 1 and keep only requested columns\n",
    "df.insert(0, \"ID\", range(1, len(df) + 1))\n",
    "df = df[[\"ID\", \"Symbol\", \"Security\", \"Wikipedia_URL\", \"Intro\"]]\n",
    "\n",
    "# 4) Export safely (quotes protect commas in text)\n",
    "out_path = \"sp500_wiki_intros_clean.csv\"\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL)\n",
    "\n",
    "print(f\"✅ Cleaned and saved {len(df)} rows to {out_path}\")\n",
    "print(\"Unique symbols:\", df['Symbol'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52b028b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 1 rows have empty Intro text. They will still be embedded (but may be low-information).\n",
      "Loaded 503 rows from sp500_wiki_intros_clean.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|██████████| 32/32 [00:42<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings matrix shape: (503, 1536)\n",
      "L2-normalized embeddings (for cosine similarity).\n",
      "Saved embeddings matrix to embeddings\\embeddings.npy\n",
      "Saved embeddings index to embeddings\\embeddings_index.csv\n",
      "All counts consistent. Ready for clustering/visualization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %% Step 1: prepare text, create embeddings, and store them with an index\n",
    "# pip install openai numpy pandas tqdm\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# -------- CONFIG --------\n",
    "CSV_CANDIDATES = [\"sp500_wiki_intros_clean.csv\", \"sp500_wiki_intros.csv\", \"sp500_wiki_intros_cleaned.csv\"]\n",
    "EMB_DIR = \"embeddings\"\n",
    "EMB_MATRIX_PATH = os.path.join(EMB_DIR, \"embeddings.npy\")\n",
    "EMB_INDEX_CSV = os.path.join(EMB_DIR, \"embeddings_index.csv\")\n",
    "MODEL = \"text-embedding-3-small\"   # cost-effective default (1536d)\n",
    "BATCH_SIZE = 16\n",
    "SLEEP_BETWEEN_BATCHES = 0.15\n",
    "MAX_RETRIES = 5\n",
    "NORMALIZE = True   # L2-normalize embeddings (recommended for cosine similarity)\n",
    "# ------------------------\n",
    "\n",
    "os.makedirs(EMB_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Load CSV (try common names) ------------------------------------------------\n",
    "csv_path = None\n",
    "for p in CSV_CANDIDATES:\n",
    "    if os.path.exists(p):\n",
    "        csv_path = p\n",
    "        break\n",
    "if csv_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Couldn't find your CSV. Make sure your cleaned CSV (sp500_wiki_intros_clean.csv or similar) is in the notebook working directory.\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "required_cols = {\"ID\", \"Symbol\", \"Security\", \"Wikipedia_URL\", \"Intro\"}\n",
    "if not required_cols.issubset(set(df.columns)):\n",
    "    raise RuntimeError(f\"CSV must include columns: {required_cols}. Found: {list(df.columns)}\")\n",
    "\n",
    "# remove rows with empty intro (or set fallback)\n",
    "df[\"Intro\"] = df[\"Intro\"].fillna(\"\").astype(str)\n",
    "empty_count = (df[\"Intro\"].str.strip() == \"\").sum()\n",
    "if empty_count > 0:\n",
    "    print(f\"Warning: {empty_count} rows have empty Intro text. They will still be embedded (but may be low-information).\")\n",
    "\n",
    "N = len(df)\n",
    "print(f\"Loaded {N} rows from {csv_path}\")\n",
    "\n",
    "# 2) Setup OpenAI client -------------------------------------------------------\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY environment variable not set. Set it securely before running.\")\n",
    "client = OpenAI()\n",
    "\n",
    "# 3) Helper: call embeddings with retries --------------------------------------\n",
    "def embed_batch(texts, model=MODEL, max_retries=MAX_RETRIES):\n",
    "    attempt = 0\n",
    "    backoff = 1.0\n",
    "    while attempt <= max_retries:\n",
    "        try:\n",
    "            resp = client.embeddings.create(model=model, input=texts)\n",
    "            vectors = [item.embedding for item in resp.data]\n",
    "            return vectors\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            if attempt > max_retries:\n",
    "                raise\n",
    "            wait = backoff * (2 ** (attempt - 1))\n",
    "            print(f\"Embedding batch failed (attempt {attempt}/{max_retries}). Retrying in {wait:.1f}s. Error: {e}\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(\"Unreachable: embed_batch failed after retries\")\n",
    "\n",
    "# 4) Batch embedding loop -----------------------------------------------------\n",
    "all_vectors = []\n",
    "index_rows = []   # metadata: vector_index -> csv row\n",
    "texts = df[\"Intro\"].tolist()\n",
    "\n",
    "for i in tqdm(range(0, N, BATCH_SIZE), desc=\"Embedding batches\"):\n",
    "    batch_indices = list(range(i, min(i + BATCH_SIZE, N)))\n",
    "    batch_texts = [texts[idx] if texts[idx].strip() != \"\" else \" \" for idx in batch_indices]\n",
    "    vectors = embed_batch(batch_texts, model=MODEL)\n",
    "    # ensure vectors length matches input\n",
    "    if len(vectors) != len(batch_texts):\n",
    "        raise RuntimeError(\"Embedding API returned unexpected number of vectors.\")\n",
    "    # append and store mapping\n",
    "    for j, vec in zip(batch_indices, vectors):\n",
    "        all_vectors.append(np.array(vec, dtype=np.float32))\n",
    "        index_rows.append({\n",
    "            \"vector_index\": len(all_vectors)-1,\n",
    "            \"ID\": int(df.loc[j, \"ID\"]),\n",
    "            \"Symbol\": df.loc[j, \"Symbol\"],\n",
    "            \"Security\": df.loc[j, \"Security\"],\n",
    "            \"Wikipedia_URL\": df.loc[j, \"Wikipedia_URL\"],\n",
    "        })\n",
    "    time.sleep(SLEEP_BETWEEN_BATCHES)\n",
    "\n",
    "# 5) Build embeddings matrix & normalize (optional) ----------------------------\n",
    "if not all_vectors:\n",
    "    raise RuntimeError(\"No embeddings produced.\")\n",
    "\n",
    "emb_matrix = np.stack(all_vectors)   # shape (N, D)\n",
    "print(\"Embeddings matrix shape:\", emb_matrix.shape)\n",
    "\n",
    "if NORMALIZE:\n",
    "    norms = np.linalg.norm(emb_matrix, axis=1, keepdims=True)\n",
    "    norms[norms == 0.0] = 1.0\n",
    "    emb_matrix = emb_matrix / norms\n",
    "    print(\"L2-normalized embeddings (for cosine similarity).\")\n",
    "\n",
    "# 6) Persist matrix + index ---------------------------------------------------\n",
    "np.save(EMB_MATRIX_PATH, emb_matrix)\n",
    "print(\"Saved embeddings matrix to\", EMB_MATRIX_PATH)\n",
    "\n",
    "# write index CSV (maps vector_index -> company metadata)\n",
    "with open(EMB_INDEX_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"vector_index\", \"ID\", \"Symbol\", \"Security\", \"Wikipedia_URL\"])\n",
    "    writer.writeheader()\n",
    "    for r in index_rows:\n",
    "        writer.writerow(r)\n",
    "print(\"Saved embeddings index to\", EMB_INDEX_CSV)\n",
    "\n",
    "# 7) Basic sanity checks ------------------------------------------------------\n",
    "if emb_matrix.shape[0] != len(index_rows) or emb_matrix.shape[0] != N:\n",
    "    print(\"Warning: counts mismatch. embedding rows:\", emb_matrix.shape[0], \"index rows:\", len(index_rows), \"CSV rows:\", N)\n",
    "else:\n",
    "    print(\"All counts consistent. Ready for clustering/visualization.\")\n",
    "\n",
    "# Quick example: load back to verify\n",
    "# loaded = np.load(EMB_MATRIX_PATH)\n",
    "# print('loaded shape', loaded.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
