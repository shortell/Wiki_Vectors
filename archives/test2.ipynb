{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83ad3f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacks\\AppData\\Local\\Temp\\ipykernel_24488\\4091008496.py:16: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "Fetching intros: 100%|██████████| 503/503 [03:20<00:00,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before dedupe: 503, after dedupe: 503 (unique symbols)\n",
      "✅ Exported 503 rows to sp500_wiki_intros_clean.csv\n",
      "Unique symbols: 503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Step 1: Get S&P 500 companies table\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; sp500-fetcher/1.0; +https://example.com)\"}\n",
    "resp = requests.get(url, headers=headers, timeout=20)\n",
    "resp.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "df = pd.read_html(str(table))[0]\n",
    "\n",
    "# Step 2: Get company Wikipedia URLs\n",
    "base = \"https://en.wikipedia.org\"\n",
    "links = [base + a[\"href\"] for a in table.select(\"tbody tr td:nth-of-type(2) a[href]\")]\n",
    "df = df.iloc[:len(links)].copy()\n",
    "df[\"Wikipedia_URL\"] = links\n",
    "\n",
    "# Step 3: Fetch short intro for each company\n",
    "def get_intro(url):\n",
    "    title = url.split(\"/wiki/\")[-1]\n",
    "    api_url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{title}\"\n",
    "    try:\n",
    "        r = requests.get(api_url, headers=headers, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        return r.json().get(\"extract\", \"\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "intros = []\n",
    "for link in tqdm(df[\"Wikipedia_URL\"], desc=\"Fetching intros\"):\n",
    "    intros.append(get_intro(link))\n",
    "    time.sleep(0.3)  # delay to avoid being blocked\n",
    "\n",
    "df[\"Intro\"] = intros\n",
    "\n",
    "# Step 4: Clean and deduplicate data\n",
    "df = df[[\"Symbol\", \"Security\", \"Wikipedia_URL\", \"Intro\"]].copy()\n",
    "df[\"Symbol\"] = df[\"Symbol\"].astype(str).str.strip()\n",
    "df[\"Security\"] = df[\"Security\"].astype(str).str.strip()\n",
    "df = df[df[\"Symbol\"] != \"\"]\n",
    "df = df.dropna(subset=[\"Symbol\"])\n",
    "\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=[\"Symbol\"], keep=\"first\").reset_index(drop=True)\n",
    "after = len(df)\n",
    "\n",
    "print(f\"Rows before dedupe: {before}, after dedupe: {after} (unique symbols)\")\n",
    "\n",
    "# Step 5: Add ID and finalize column order\n",
    "df.insert(0, \"ID\", range(1, len(df) + 1))\n",
    "\n",
    "# Step 6: Save to CSV with proper quoting\n",
    "out_path = \"sp500_wiki_intros_clean.csv\"\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL)\n",
    "\n",
    "print(f\"✅ Exported {len(df)} rows to {out_path}\")\n",
    "print(f\"Unique symbols: {df['Symbol'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b8ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the cleaned CSV\n",
    "df = pd.read_csv(\"sp500_wiki_intros_clean.csv\")\n",
    "\n",
    "print(f\"Loaded {len(df)} companies\")\n",
    "\n",
    "# Prep intro text for embeddings\n",
    "def prep_text_for_embedding(text):\n",
    "    \"\"\"Clean and normalize text for vector embedding\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters but keep basic punctuation\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\-]', '', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df[\"Intro_Embedding_Ready\"] = df[\"Intro\"].apply(prep_text_for_embedding)\n",
    "\n",
    "# Remove rows with empty intros\n",
    "df_ready = df[df[\"Intro_Embedding_Ready\"] != \"\"].copy()\n",
    "\n",
    "print(f\"✅ Prepared {len(df_ready)} intro texts for embedding\")\n",
    "print(f\"Dropped {len(df) - len(df_ready)} rows with empty intros\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample preprocessed text:\")\n",
    "print(df_ready[[\"Symbol\", \"Security\", \"Intro_Embedding_Ready\"]].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde178e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client with API key from .env\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Configuration\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"  # or \"text-embedding-3-large\" for higher quality\n",
    "BATCH_SIZE = 100  # OpenAI allows up to 2048 texts per request, but smaller batches are safer\n",
    "\n",
    "def get_embeddings_batch(texts, model=EMBEDDING_MODEL):\n",
    "    \"\"\"Get embeddings for a batch of texts\"\"\"\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            input=texts,\n",
    "            model=model\n",
    "        )\n",
    "        return [item.embedding for item in response.data]\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embeddings: {e}\")\n",
    "        return None\n",
    "\n",
    "# Prepare data for embedding\n",
    "texts_to_embed = df_ready[\"Intro_Embedding_Ready\"].tolist()\n",
    "total_texts = len(texts_to_embed)\n",
    "\n",
    "print(f\"Creating embeddings for {total_texts} companies using {EMBEDDING_MODEL}...\")\n",
    "\n",
    "# Process in batches\n",
    "all_embeddings = []\n",
    "for i in tqdm(range(0, total_texts, BATCH_SIZE)):\n",
    "    batch = texts_to_embed[i:i + BATCH_SIZE]\n",
    "    embeddings = get_embeddings_batch(batch)\n",
    "    \n",
    "    if embeddings:\n",
    "        all_embeddings.extend(embeddings)\n",
    "    else:\n",
    "        # If batch fails, try one at a time\n",
    "        for text in batch:\n",
    "            emb = get_embeddings_batch([text])\n",
    "            all_embeddings.extend(emb if emb else [[0] * 1536])  # fallback to zero vector\n",
    "    \n",
    "    time.sleep(0.2)  # Rate limiting\n",
    "\n",
    "# Add embeddings to dataframe\n",
    "df_ready[\"Embedding\"] = all_embeddings\n",
    "\n",
    "# Create a lookup dictionary for easy access\n",
    "embedding_lookup = dict(zip(df_ready[\"Symbol\"], df_ready[\"Embedding\"]))\n",
    "\n",
    "print(f\"✅ Created {len(all_embeddings)} embeddings\")\n",
    "print(f\"Embedding dimensions: {len(all_embeddings[0])}\")\n",
    "print(f\"\\nSample companies with embeddings:\")\n",
    "print(df_ready[[\"ID\", \"Symbol\", \"Security\"]].head())\n",
    "\n",
    "# Optional: Save embeddings to numpy file for later use\n",
    "np.save(\"sp500_embeddings.npy\", np.array(all_embeddings))\n",
    "df_ready[[\"ID\", \"Symbol\", \"Security\", \"Wikipedia_URL\"]].to_csv(\"sp500_embedding_metadata.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ Saved embeddings to sp500_embeddings.npy\")\n",
    "print(\"✅ Saved metadata to sp500_embedding_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fdc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load embeddings if not already in memory\n",
    "if 'all_embeddings' not in locals() or not isinstance(all_embeddings, np.ndarray):\n",
    "    all_embeddings = np.load(\"sp500_embeddings.npy\")\n",
    "    df_ready = pd.read_csv(\"sp500_embedding_metadata.csv\")\n",
    "\n",
    "# Convert to numpy array if it's a list\n",
    "if isinstance(all_embeddings, list):\n",
    "    all_embeddings = np.array(all_embeddings)\n",
    "\n",
    "print(f\"Original embedding shape: {all_embeddings.shape}\")\n",
    "\n",
    "# Step 1: Normalize embeddings (optional but often helps with clustering)\n",
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(all_embeddings)\n",
    "\n",
    "# Step 2: Optional dimensionality reduction with PCA\n",
    "# This can help HDBSCAN perform better and run faster\n",
    "n_components = 50  # Reduce from 1536 to 50 dimensions\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "embeddings_reduced = pca.fit_transform(embeddings_scaled)\n",
    "\n",
    "print(f\"Reduced embedding shape: {embeddings_reduced.shape}\")\n",
    "print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Step 3: Perform HDBSCAN clustering\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=5,        # Minimum points to form a cluster\n",
    "    min_samples=3,              # Conservative core point threshold\n",
    "    metric='euclidean',\n",
    "    cluster_selection_epsilon=0.0,\n",
    "    cluster_selection_method='eom'\n",
    ")\n",
    "\n",
    "print(\"\\nPerforming HDBSCAN clustering...\")\n",
    "cluster_labels = clusterer.fit_predict(embeddings_reduced)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df_ready[\"Cluster\"] = cluster_labels\n",
    "\n",
    "# Analyze clustering results\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = list(cluster_labels).count(-1)\n",
    "\n",
    "print(f\"\\n✅ Clustering complete!\")\n",
    "print(f\"Number of clusters found: {n_clusters}\")\n",
    "print(f\"Number of noise points (cluster -1): {n_noise}\")\n",
    "print(f\"Clustered companies: {len(df_ready) - n_noise}\")\n",
    "\n",
    "# Show cluster distribution\n",
    "cluster_counts = df_ready['Cluster'].value_counts().sort_index()\n",
    "print(\"\\nCluster sizes:\")\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    if cluster_id == -1:\n",
    "        print(f\"  Noise (cluster -1): {count} companies\")\n",
    "    else:\n",
    "        print(f\"  Cluster {cluster_id}: {count} companies\")\n",
    "\n",
    "# Display sample companies from each cluster\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample companies from each cluster:\")\n",
    "print(\"=\"*80)\n",
    "for cluster_id in sorted(df_ready['Cluster'].unique()):\n",
    "    if cluster_id == -1:\n",
    "        continue\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    sample = df_ready[df_ready['Cluster'] == cluster_id][['Symbol', 'Security']].head(10)\n",
    "    for idx, row in sample.iterrows():\n",
    "        print(f\"  • {row['Symbol']}: {row['Security']}\")\n",
    "\n",
    "# Visualize clustering with 2D projection\n",
    "pca_2d = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca_2d.fit_transform(embeddings_scaled)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "scatter = plt.scatter(\n",
    "    embeddings_2d[:, 0], \n",
    "    embeddings_2d[:, 1], \n",
    "    c=cluster_labels, \n",
    "    cmap='tab20', \n",
    "    alpha=0.6,\n",
    "    s=50\n",
    ")\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title(f'S&P 500 Company Clustering (HDBSCAN)\\n{n_clusters} clusters, {n_noise} noise points')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save clustering results\n",
    "df_ready.to_csv(\"sp500_clustered.csv\", index=False)\n",
    "print(f\"\\n✅ Saved clustered data to sp500_clustered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06814830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Use the 2D PCA projection from before (or recreate if needed)\n",
    "if 'embeddings_2d' not in locals():\n",
    "    pca_2d = PCA(n_components=2, random_state=42)\n",
    "    embeddings_2d = pca_2d.fit_transform(embeddings_scaled)\n",
    "\n",
    "# Add 2D coordinates to dataframe\n",
    "df_ready['PC1'] = embeddings_2d[:, 0]\n",
    "df_ready['PC2'] = embeddings_2d[:, 1]\n",
    "\n",
    "# Create hover text with company info\n",
    "df_ready['hover_text'] = (\n",
    "    df_ready['Security'] + ' (' + df_ready['Symbol'] + ')' +\n",
    "    '<br>Cluster: ' + df_ready['Cluster'].astype(str)\n",
    ")\n",
    "\n",
    "# Convert cluster to string for better color mapping\n",
    "df_ready['Cluster_Label'] = df_ready['Cluster'].apply(\n",
    "    lambda x: 'Noise' if x == -1 else f'Cluster {x}'\n",
    ")\n",
    "\n",
    "# Create interactive scatter plot\n",
    "fig = px.scatter(\n",
    "    df_ready,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    color='Cluster_Label',\n",
    "    hover_name='Security',\n",
    "    hover_data={\n",
    "        'Symbol': True,\n",
    "        'Cluster': True,\n",
    "        'PC1': False,\n",
    "        'PC2': False,\n",
    "        'Cluster_Label': False\n",
    "    },\n",
    "    title=f'Interactive S&P 500 Company Clustering<br><sub>{n_clusters} clusters found, {n_noise} noise points</sub>',\n",
    "    labels={'PC1': 'First Principal Component', 'PC2': 'Second Principal Component'},\n",
    "    color_discrete_sequence=px.colors.qualitative.Light24,\n",
    "    width=1200,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Customize markers\n",
    "fig.update_traces(\n",
    "    marker=dict(size=8, line=dict(width=0.5, color='white')),\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    hovermode='closest',\n",
    "    plot_bgcolor='white',\n",
    "    legend_title_text='Clusters',\n",
    "    font=dict(size=12),\n",
    "    xaxis=dict(showgrid=True, gridcolor='lightgray'),\n",
    "    yaxis=dict(showgrid=True, gridcolor='lightgray')\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Optional: Save as HTML for sharing\n",
    "fig.write_html(\"sp500_clustering_interactive.html\")\n",
    "print(\"✅ Saved interactive plot to sp500_clustering_interactive.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fbe83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "all_embeddings = np.array(np.load(\"sp500_embeddings.npy\"))\n",
    "df_ready = pd.read_csv(\"sp500_embedding_metadata.csv\")\n",
    "\n",
    "print(f\"Original data: {all_embeddings.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Enhanced Dimensionality Reduction\n",
    "# ============================================================================\n",
    "# Try UMAP instead of just PCA - it preserves local structure better for clustering\n",
    "\n",
    "print(\"\\n--- Dimensionality Reduction ---\")\n",
    "\n",
    "# First use PCA to reduce to manageable size (removes noise)\n",
    "pca_initial = PCA(n_components=100, random_state=42)\n",
    "embeddings_pca = pca_initial.fit_transform(all_embeddings)\n",
    "print(f\"PCA to 100 dims - Explained variance: {pca_initial.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Then use UMAP for better cluster separation\n",
    "umap_reducer = umap.UMAP(\n",
    "    n_neighbors=15,        # Balance between local and global structure\n",
    "    min_dist=0.1,          # Minimum distance between points\n",
    "    n_components=10,       # Reduce to 10 dimensions for clustering\n",
    "    metric='cosine',       # Cosine distance works well for embeddings\n",
    "    random_state=42\n",
    ")\n",
    "embeddings_umap = umap_reducer.fit_transform(embeddings_pca)\n",
    "print(f\"UMAP to 10 dims - Complete\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Normalize for clustering\n",
    "# ============================================================================\n",
    "scaler = StandardScaler()\n",
    "embeddings_normalized = scaler.fit_transform(embeddings_umap)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Optimized HDBSCAN Parameters\n",
    "# ============================================================================\n",
    "print(\"\\n--- HDBSCAN Clustering ---\")\n",
    "\n",
    "# More aggressive parameters to reduce noise\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=8,              # Smaller = more clusters, less noise\n",
    "    min_samples=2,                   # Lower = more lenient clustering\n",
    "    metric='euclidean',\n",
    "    cluster_selection_epsilon=0.5,   # Allow merging of close clusters\n",
    "    cluster_selection_method='eom',  # Excess of Mass method\n",
    "    alpha=1.0                        # Default linkage strength\n",
    ")\n",
    "\n",
    "cluster_labels = clusterer.fit_predict(embeddings_normalized)\n",
    "df_ready[\"Cluster\"] = cluster_labels\n",
    "\n",
    "# Calculate clustering quality metrics\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = list(cluster_labels).count(-1)\n",
    "noise_pct = (n_noise / len(cluster_labels)) * 100\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Noise points: {n_noise} ({noise_pct:.1f}%)\")\n",
    "print(f\"Clustered companies: {len(df_ready) - n_noise}\")\n",
    "\n",
    "# Cluster persistence scores (higher = more stable clusters)\n",
    "if hasattr(clusterer, 'cluster_persistence_'):\n",
    "    print(\"\\nCluster stability scores:\")\n",
    "    for i, score in enumerate(clusterer.cluster_persistence_):\n",
    "        print(f\"  Cluster {i}: {score:.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Analyze Cluster Quality\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Cluster Analysis:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cluster_counts = df_ready['Cluster'].value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    if cluster_id == -1:\n",
    "        print(f\"\\nNoise (cluster -1): {count} companies\")\n",
    "    else:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Cluster {cluster_id}: {count} companies\")\n",
    "        print(f\"{'='*60}\")\n",
    "        sample = df_ready[df_ready['Cluster'] == cluster_id][['Symbol', 'Security']].head(15)\n",
    "        for idx, row in sample.iterrows():\n",
    "            print(f\"  • {row['Symbol']}: {row['Security']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Create 2D Visualization with UMAP\n",
    "# ============================================================================\n",
    "print(\"\\n--- Creating 2D Visualization ---\")\n",
    "\n",
    "# Use UMAP for 2D visualization (better than PCA for clusters)\n",
    "umap_2d = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.3,\n",
    "    n_components=2,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "embeddings_2d = umap_2d.fit_transform(embeddings_pca)\n",
    "\n",
    "df_ready['X'] = embeddings_2d[:, 0]\n",
    "df_ready['Y'] = embeddings_2d[:, 1]\n",
    "\n",
    "# Enhanced matplotlib visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot 1: All clusters\n",
    "scatter1 = ax1.scatter(\n",
    "    embeddings_2d[:, 0],\n",
    "    embeddings_2d[:, 1],\n",
    "    c=cluster_labels,\n",
    "    cmap='tab20',\n",
    "    alpha=0.7,\n",
    "    s=60,\n",
    "    edgecolors='white',\n",
    "    linewidth=0.5\n",
    ")\n",
    "ax1.set_title(f'S&P 500 Clustering (UMAP + HDBSCAN)\\n{n_clusters} clusters, {noise_pct:.1f}% noise', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('UMAP Dimension 1', fontsize=12)\n",
    "ax1.set_ylabel('UMAP Dimension 2', fontsize=12)\n",
    "plt.colorbar(scatter1, ax=ax1, label='Cluster')\n",
    "\n",
    "# Plot 2: Without noise points\n",
    "non_noise = cluster_labels != -1\n",
    "scatter2 = ax2.scatter(\n",
    "    embeddings_2d[non_noise, 0],\n",
    "    embeddings_2d[non_noise, 1],\n",
    "    c=cluster_labels[non_noise],\n",
    "    cmap='tab20',\n",
    "    alpha=0.7,\n",
    "    s=60,\n",
    "    edgecolors='white',\n",
    "    linewidth=0.5\n",
    ")\n",
    "ax2.set_title(f'Clustered Companies Only\\n{len(df_ready) - n_noise} companies in {n_clusters} clusters', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('UMAP Dimension 1', fontsize=12)\n",
    "ax2.set_ylabel('UMAP Dimension 2', fontsize=12)\n",
    "plt.colorbar(scatter2, ax=ax2, label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "df_ready.to_csv(\"sp500_clustered_improved.csv\", index=False)\n",
    "print(f\"\\n✅ Saved improved clustering to sp500_clustered_improved.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Parameter Tuning Suggestions\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TUNING SUGGESTIONS:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Current noise level: {noise_pct:.1f}%\")\n",
    "print(\"\\nIf you want FEWER noise points (more companies clustered):\")\n",
    "print(\"  • Decrease min_cluster_size (try 5-7)\")\n",
    "print(\"  • Decrease min_samples (try 1-2)\")\n",
    "print(\"  • Increase cluster_selection_epsilon (try 0.8-1.0)\")\n",
    "print(\"\\nIf you want TIGHTER, MORE DISTINCT clusters:\")\n",
    "print(\"  • Increase min_cluster_size (try 10-15)\")\n",
    "print(\"  • Increase min_samples (try 3-5)\")\n",
    "print(\"  • Decrease cluster_selection_epsilon (try 0.0-0.3)\")\n",
    "print(\"\\nIf clusters look too merged:\")\n",
    "print(\"  • Try different n_neighbors in UMAP (try 5-30)\")\n",
    "print(\"  • Adjust min_dist in UMAP (try 0.0-0.5)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
