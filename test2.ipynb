{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad3f0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m soup = BeautifulSoup(StringIO(resp.text), \u001b[33m\"\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m table = soup.find(\u001b[33m\"\u001b[39m\u001b[33mtable\u001b[39m\u001b[33m\"\u001b[39m, {\u001b[33m\"\u001b[39m\u001b[33mclass\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mwikitable\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Step 2: Get company Wikipedia URLs\u001b[39;00m\n\u001b[32m     20\u001b[39m base = \u001b[33m\"\u001b[39m\u001b[33mhttps://en.wikipedia.org\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Wiki_Vectors/venv/lib/python3.12/site-packages/pandas/io/html.py:1240\u001b[39m, in \u001b[36mread_html\u001b[39m\u001b[34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[39m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   1225\u001b[39m     [\n\u001b[32m   1226\u001b[39m         is_file_like(io),\n\u001b[32m   (...)\u001b[39m\u001b[32m   1230\u001b[39m     ]\n\u001b[32m   1231\u001b[39m ):\n\u001b[32m   1232\u001b[39m     warnings.warn(\n\u001b[32m   1233\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing literal html to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mread_html\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1234\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future version. To read from a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Wiki_Vectors/venv/lib/python3.12/site-packages/pandas/io/html.py:983\u001b[39m, in \u001b[36m_parse\u001b[39m\u001b[34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[39m\n\u001b[32m    972\u001b[39m p = parser(\n\u001b[32m    973\u001b[39m     io,\n\u001b[32m    974\u001b[39m     compiled_match,\n\u001b[32m   (...)\u001b[39m\u001b[32m    979\u001b[39m     storage_options,\n\u001b[32m    980\u001b[39m )\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m     tables = \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[32m    985\u001b[39m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[32m    986\u001b[39m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[33m\"\u001b[39m\u001b[33mseekable\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io.seekable():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Wiki_Vectors/venv/lib/python3.12/site-packages/pandas/io/html.py:249\u001b[39m, in \u001b[36m_HtmlFrameParser.parse_tables\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    242\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[33;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[32m    244\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m \u001b[33;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     tables = \u001b[38;5;28mself\u001b[39m._parse_tables(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.match, \u001b[38;5;28mself\u001b[39m.attrs)\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Wiki_Vectors/venv/lib/python3.12/site-packages/pandas/io/html.py:791\u001b[39m, in \u001b[36m_LxmlFrameParser._build_doc\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m         r = parse(f.handle, parser=parser)\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    790\u001b[39m     \u001b[38;5;66;03m# try to parse the input in the simplest way\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m     r = \u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    793\u001b[39m     r = r.getroot()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Wiki_Vectors/venv/lib/python3.12/site-packages/lxml/html/__init__.py:918\u001b[39m, in \u001b[36mparse\u001b[39m\u001b[34m(filename_or_url, parser, base_url, **kw)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     parser = html_parser\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43metree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/lxml/etree.pyx:3711\u001b[39m, in \u001b[36mlxml.etree.parse\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/lxml/parser.pxi:2047\u001b[39m, in \u001b[36mlxml.etree._parseDocument\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Step 1: Get S&P 500 companies table\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; sp500-fetcher/1.0; +https://example.com)\"}\n",
    "resp = requests.get(url, headers=headers, timeout=20)\n",
    "resp.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "df = pd.read_html(str(table))[0]\n",
    "\n",
    "# Step 2: Get company Wikipedia URLs\n",
    "base = \"https://en.wikipedia.org\"\n",
    "links = [base + a[\"href\"] for a in table.select(\"tbody tr td:nth-of-type(2) a[href]\")]\n",
    "df = df.iloc[:len(links)].copy()\n",
    "df[\"Wikipedia_URL\"] = links\n",
    "\n",
    "# Step 3: Fetch short intro for each company\n",
    "def get_intro(url):\n",
    "    title = url.split(\"/wiki/\")[-1]\n",
    "    api_url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{title}\"\n",
    "    try:\n",
    "        r = requests.get(api_url, headers=headers, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        return r.json().get(\"extract\", \"\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "intros = []\n",
    "for link in tqdm(df[\"Wikipedia_URL\"], desc=\"Fetching intros\"):\n",
    "    intros.append(get_intro(link))\n",
    "    time.sleep(0.3)  # delay to avoid being blocked\n",
    "\n",
    "df[\"Intro\"] = intros\n",
    "\n",
    "# Step 4: Clean and deduplicate data\n",
    "df = df[[\"Symbol\", \"Security\", \"Wikipedia_URL\", \"Intro\"]].copy()\n",
    "df[\"Symbol\"] = df[\"Symbol\"].astype(str).str.strip()\n",
    "df[\"Security\"] = df[\"Security\"].astype(str).str.strip()\n",
    "df = df[df[\"Symbol\"] != \"\"]\n",
    "df = df.dropna(subset=[\"Symbol\"])\n",
    "\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=[\"Symbol\"], keep=\"first\").reset_index(drop=True)\n",
    "after = len(df)\n",
    "\n",
    "print(f\"Rows before dedupe: {before}, after dedupe: {after} (unique symbols)\")\n",
    "\n",
    "# Step 5: Add ID and finalize column order\n",
    "df.insert(0, \"ID\", range(1, len(df) + 1))\n",
    "\n",
    "# Step 6: Save to CSV with proper quoting\n",
    "out_path = \"sp500_wiki_intros_clean.csv\"\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL)\n",
    "\n",
    "print(f\"✅ Exported {len(df)} rows to {out_path}\")\n",
    "print(f\"Unique symbols: {df['Symbol'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b8ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the cleaned CSV\n",
    "df = pd.read_csv(\"sp500_wiki_intros_clean.csv\")\n",
    "\n",
    "print(f\"Loaded {len(df)} companies\")\n",
    "\n",
    "# Prep intro text for embeddings\n",
    "def prep_text_for_embedding(text):\n",
    "    \"\"\"Clean and normalize text for vector embedding\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters but keep basic punctuation\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\-]', '', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df[\"Intro_Embedding_Ready\"] = df[\"Intro\"].apply(prep_text_for_embedding)\n",
    "\n",
    "# Remove rows with empty intros\n",
    "df_ready = df[df[\"Intro_Embedding_Ready\"] != \"\"].copy()\n",
    "\n",
    "print(f\"✅ Prepared {len(df_ready)} intro texts for embedding\")\n",
    "print(f\"Dropped {len(df) - len(df_ready)} rows with empty intros\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample preprocessed text:\")\n",
    "print(df_ready[[\"Symbol\", \"Security\", \"Intro_Embedding_Ready\"]].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde178e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client with API key from .env\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Configuration\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"  # or \"text-embedding-3-large\" for higher quality\n",
    "BATCH_SIZE = 100  # OpenAI allows up to 2048 texts per request, but smaller batches are safer\n",
    "\n",
    "def get_embeddings_batch(texts, model=EMBEDDING_MODEL):\n",
    "    \"\"\"Get embeddings for a batch of texts\"\"\"\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            input=texts,\n",
    "            model=model\n",
    "        )\n",
    "        return [item.embedding for item in response.data]\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embeddings: {e}\")\n",
    "        return None\n",
    "\n",
    "# Prepare data for embedding\n",
    "texts_to_embed = df_ready[\"Intro_Embedding_Ready\"].tolist()\n",
    "total_texts = len(texts_to_embed)\n",
    "\n",
    "print(f\"Creating embeddings for {total_texts} companies using {EMBEDDING_MODEL}...\")\n",
    "\n",
    "# Process in batches\n",
    "all_embeddings = []\n",
    "for i in tqdm(range(0, total_texts, BATCH_SIZE)):\n",
    "    batch = texts_to_embed[i:i + BATCH_SIZE]\n",
    "    embeddings = get_embeddings_batch(batch)\n",
    "    \n",
    "    if embeddings:\n",
    "        all_embeddings.extend(embeddings)\n",
    "    else:\n",
    "        # If batch fails, try one at a time\n",
    "        for text in batch:\n",
    "            emb = get_embeddings_batch([text])\n",
    "            all_embeddings.extend(emb if emb else [[0] * 1536])  # fallback to zero vector\n",
    "    \n",
    "    time.sleep(0.2)  # Rate limiting\n",
    "\n",
    "# Add embeddings to dataframe\n",
    "df_ready[\"Embedding\"] = all_embeddings\n",
    "\n",
    "# Create a lookup dictionary for easy access\n",
    "embedding_lookup = dict(zip(df_ready[\"Symbol\"], df_ready[\"Embedding\"]))\n",
    "\n",
    "print(f\"✅ Created {len(all_embeddings)} embeddings\")\n",
    "print(f\"Embedding dimensions: {len(all_embeddings[0])}\")\n",
    "print(f\"\\nSample companies with embeddings:\")\n",
    "print(df_ready[[\"ID\", \"Symbol\", \"Security\"]].head())\n",
    "\n",
    "# Optional: Save embeddings to numpy file for later use\n",
    "np.save(\"sp500_embeddings.npy\", np.array(all_embeddings))\n",
    "df_ready[[\"ID\", \"Symbol\", \"Security\", \"Wikipedia_URL\"]].to_csv(\"sp500_embedding_metadata.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ Saved embeddings to sp500_embeddings.npy\")\n",
    "print(\"✅ Saved metadata to sp500_embedding_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fdc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load embeddings if not already in memory\n",
    "if 'all_embeddings' not in locals() or not isinstance(all_embeddings, np.ndarray):\n",
    "    all_embeddings = np.load(\"sp500_embeddings.npy\")\n",
    "    df_ready = pd.read_csv(\"sp500_embedding_metadata.csv\")\n",
    "\n",
    "# Convert to numpy array if it's a list\n",
    "if isinstance(all_embeddings, list):\n",
    "    all_embeddings = np.array(all_embeddings)\n",
    "\n",
    "print(f\"Original embedding shape: {all_embeddings.shape}\")\n",
    "\n",
    "# Step 1: Normalize embeddings (optional but often helps with clustering)\n",
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(all_embeddings)\n",
    "\n",
    "# Step 2: Optional dimensionality reduction with PCA\n",
    "# This can help HDBSCAN perform better and run faster\n",
    "n_components = 50  # Reduce from 1536 to 50 dimensions\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "embeddings_reduced = pca.fit_transform(embeddings_scaled)\n",
    "\n",
    "print(f\"Reduced embedding shape: {embeddings_reduced.shape}\")\n",
    "print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Step 3: Perform HDBSCAN clustering\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=5,        # Minimum points to form a cluster\n",
    "    min_samples=3,              # Conservative core point threshold\n",
    "    metric='euclidean',\n",
    "    cluster_selection_epsilon=0.0,\n",
    "    cluster_selection_method='eom'\n",
    ")\n",
    "\n",
    "print(\"\\nPerforming HDBSCAN clustering...\")\n",
    "cluster_labels = clusterer.fit_predict(embeddings_reduced)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df_ready[\"Cluster\"] = cluster_labels\n",
    "\n",
    "# Analyze clustering results\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = list(cluster_labels).count(-1)\n",
    "\n",
    "print(f\"\\n✅ Clustering complete!\")\n",
    "print(f\"Number of clusters found: {n_clusters}\")\n",
    "print(f\"Number of noise points (cluster -1): {n_noise}\")\n",
    "print(f\"Clustered companies: {len(df_ready) - n_noise}\")\n",
    "\n",
    "# Show cluster distribution\n",
    "cluster_counts = df_ready['Cluster'].value_counts().sort_index()\n",
    "print(\"\\nCluster sizes:\")\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    if cluster_id == -1:\n",
    "        print(f\"  Noise (cluster -1): {count} companies\")\n",
    "    else:\n",
    "        print(f\"  Cluster {cluster_id}: {count} companies\")\n",
    "\n",
    "# Display sample companies from each cluster\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample companies from each cluster:\")\n",
    "print(\"=\"*80)\n",
    "for cluster_id in sorted(df_ready['Cluster'].unique()):\n",
    "    if cluster_id == -1:\n",
    "        continue\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    sample = df_ready[df_ready['Cluster'] == cluster_id][['Symbol', 'Security']].head(10)\n",
    "    for idx, row in sample.iterrows():\n",
    "        print(f\"  • {row['Symbol']}: {row['Security']}\")\n",
    "\n",
    "# Visualize clustering with 2D projection\n",
    "pca_2d = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca_2d.fit_transform(embeddings_scaled)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "scatter = plt.scatter(\n",
    "    embeddings_2d[:, 0], \n",
    "    embeddings_2d[:, 1], \n",
    "    c=cluster_labels, \n",
    "    cmap='tab20', \n",
    "    alpha=0.6,\n",
    "    s=50\n",
    ")\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title(f'S&P 500 Company Clustering (HDBSCAN)\\n{n_clusters} clusters, {n_noise} noise points')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save clustering results\n",
    "df_ready.to_csv(\"sp500_clustered.csv\", index=False)\n",
    "print(f\"\\n✅ Saved clustered data to sp500_clustered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06814830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Use the 2D PCA projection from before (or recreate if needed)\n",
    "if 'embeddings_2d' not in locals():\n",
    "    pca_2d = PCA(n_components=2, random_state=42)\n",
    "    embeddings_2d = pca_2d.fit_transform(embeddings_scaled)\n",
    "\n",
    "# Add 2D coordinates to dataframe\n",
    "df_ready['PC1'] = embeddings_2d[:, 0]\n",
    "df_ready['PC2'] = embeddings_2d[:, 1]\n",
    "\n",
    "# Create hover text with company info\n",
    "df_ready['hover_text'] = (\n",
    "    df_ready['Security'] + ' (' + df_ready['Symbol'] + ')' +\n",
    "    '<br>Cluster: ' + df_ready['Cluster'].astype(str)\n",
    ")\n",
    "\n",
    "# Convert cluster to string for better color mapping\n",
    "df_ready['Cluster_Label'] = df_ready['Cluster'].apply(\n",
    "    lambda x: 'Noise' if x == -1 else f'Cluster {x}'\n",
    ")\n",
    "\n",
    "# Create interactive scatter plot\n",
    "fig = px.scatter(\n",
    "    df_ready,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    color='Cluster_Label',\n",
    "    hover_name='Security',\n",
    "    hover_data={\n",
    "        'Symbol': True,\n",
    "        'Cluster': True,\n",
    "        'PC1': False,\n",
    "        'PC2': False,\n",
    "        'Cluster_Label': False\n",
    "    },\n",
    "    title=f'Interactive S&P 500 Company Clustering<br><sub>{n_clusters} clusters found, {n_noise} noise points</sub>',\n",
    "    labels={'PC1': 'First Principal Component', 'PC2': 'Second Principal Component'},\n",
    "    color_discrete_sequence=px.colors.qualitative.Light24,\n",
    "    width=1200,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Customize markers\n",
    "fig.update_traces(\n",
    "    marker=dict(size=8, line=dict(width=0.5, color='white')),\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    hovermode='closest',\n",
    "    plot_bgcolor='white',\n",
    "    legend_title_text='Clusters',\n",
    "    font=dict(size=12),\n",
    "    xaxis=dict(showgrid=True, gridcolor='lightgray'),\n",
    "    yaxis=dict(showgrid=True, gridcolor='lightgray')\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Optional: Save as HTML for sharing\n",
    "fig.write_html(\"sp500_clustering_interactive.html\")\n",
    "print(\"✅ Saved interactive plot to sp500_clustering_interactive.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fbe83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "all_embeddings = np.array(np.load(\"sp500_embeddings.npy\"))\n",
    "df_ready = pd.read_csv(\"sp500_embedding_metadata.csv\")\n",
    "\n",
    "print(f\"Original data: {all_embeddings.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Enhanced Dimensionality Reduction\n",
    "# ============================================================================\n",
    "# Try UMAP instead of just PCA - it preserves local structure better for clustering\n",
    "\n",
    "print(\"\\n--- Dimensionality Reduction ---\")\n",
    "\n",
    "# First use PCA to reduce to manageable size (removes noise)\n",
    "pca_initial = PCA(n_components=100, random_state=42)\n",
    "embeddings_pca = pca_initial.fit_transform(all_embeddings)\n",
    "print(f\"PCA to 100 dims - Explained variance: {pca_initial.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Then use UMAP for better cluster separation\n",
    "umap_reducer = umap.UMAP(\n",
    "    n_neighbors=15,        # Balance between local and global structure\n",
    "    min_dist=0.1,          # Minimum distance between points\n",
    "    n_components=10,       # Reduce to 10 dimensions for clustering\n",
    "    metric='cosine',       # Cosine distance works well for embeddings\n",
    "    random_state=42\n",
    ")\n",
    "embeddings_umap = umap_reducer.fit_transform(embeddings_pca)\n",
    "print(f\"UMAP to 10 dims - Complete\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Normalize for clustering\n",
    "# ============================================================================\n",
    "scaler = StandardScaler()\n",
    "embeddings_normalized = scaler.fit_transform(embeddings_umap)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Optimized HDBSCAN Parameters\n",
    "# ============================================================================\n",
    "print(\"\\n--- HDBSCAN Clustering ---\")\n",
    "\n",
    "# More aggressive parameters to reduce noise\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=8,              # Smaller = more clusters, less noise\n",
    "    min_samples=2,                   # Lower = more lenient clustering\n",
    "    metric='euclidean',\n",
    "    cluster_selection_epsilon=0.5,   # Allow merging of close clusters\n",
    "    cluster_selection_method='eom',  # Excess of Mass method\n",
    "    alpha=1.0                        # Default linkage strength\n",
    ")\n",
    "\n",
    "cluster_labels = clusterer.fit_predict(embeddings_normalized)\n",
    "df_ready[\"Cluster\"] = cluster_labels\n",
    "\n",
    "# Calculate clustering quality metrics\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = list(cluster_labels).count(-1)\n",
    "noise_pct = (n_noise / len(cluster_labels)) * 100\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Noise points: {n_noise} ({noise_pct:.1f}%)\")\n",
    "print(f\"Clustered companies: {len(df_ready) - n_noise}\")\n",
    "\n",
    "# Cluster persistence scores (higher = more stable clusters)\n",
    "if hasattr(clusterer, 'cluster_persistence_'):\n",
    "    print(\"\\nCluster stability scores:\")\n",
    "    for i, score in enumerate(clusterer.cluster_persistence_):\n",
    "        print(f\"  Cluster {i}: {score:.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Analyze Cluster Quality\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Cluster Analysis:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cluster_counts = df_ready['Cluster'].value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    if cluster_id == -1:\n",
    "        print(f\"\\nNoise (cluster -1): {count} companies\")\n",
    "    else:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Cluster {cluster_id}: {count} companies\")\n",
    "        print(f\"{'='*60}\")\n",
    "        sample = df_ready[df_ready['Cluster'] == cluster_id][['Symbol', 'Security']].head(15)\n",
    "        for idx, row in sample.iterrows():\n",
    "            print(f\"  • {row['Symbol']}: {row['Security']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Create 2D Visualization with UMAP\n",
    "# ============================================================================\n",
    "print(\"\\n--- Creating 2D Visualization ---\")\n",
    "\n",
    "# Use UMAP for 2D visualization (better than PCA for clusters)\n",
    "umap_2d = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.3,\n",
    "    n_components=2,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "embeddings_2d = umap_2d.fit_transform(embeddings_pca)\n",
    "\n",
    "df_ready['X'] = embeddings_2d[:, 0]\n",
    "df_ready['Y'] = embeddings_2d[:, 1]\n",
    "\n",
    "# Enhanced matplotlib visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot 1: All clusters\n",
    "scatter1 = ax1.scatter(\n",
    "    embeddings_2d[:, 0],\n",
    "    embeddings_2d[:, 1],\n",
    "    c=cluster_labels,\n",
    "    cmap='tab20',\n",
    "    alpha=0.7,\n",
    "    s=60,\n",
    "    edgecolors='white',\n",
    "    linewidth=0.5\n",
    ")\n",
    "ax1.set_title(f'S&P 500 Clustering (UMAP + HDBSCAN)\\n{n_clusters} clusters, {noise_pct:.1f}% noise', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('UMAP Dimension 1', fontsize=12)\n",
    "ax1.set_ylabel('UMAP Dimension 2', fontsize=12)\n",
    "plt.colorbar(scatter1, ax=ax1, label='Cluster')\n",
    "\n",
    "# Plot 2: Without noise points\n",
    "non_noise = cluster_labels != -1\n",
    "scatter2 = ax2.scatter(\n",
    "    embeddings_2d[non_noise, 0],\n",
    "    embeddings_2d[non_noise, 1],\n",
    "    c=cluster_labels[non_noise],\n",
    "    cmap='tab20',\n",
    "    alpha=0.7,\n",
    "    s=60,\n",
    "    edgecolors='white',\n",
    "    linewidth=0.5\n",
    ")\n",
    "ax2.set_title(f'Clustered Companies Only\\n{len(df_ready) - n_noise} companies in {n_clusters} clusters', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('UMAP Dimension 1', fontsize=12)\n",
    "ax2.set_ylabel('UMAP Dimension 2', fontsize=12)\n",
    "plt.colorbar(scatter2, ax=ax2, label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "df_ready.to_csv(\"sp500_clustered_improved.csv\", index=False)\n",
    "print(f\"\\n✅ Saved improved clustering to sp500_clustered_improved.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Parameter Tuning Suggestions\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TUNING SUGGESTIONS:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Current noise level: {noise_pct:.1f}%\")\n",
    "print(\"\\nIf you want FEWER noise points (more companies clustered):\")\n",
    "print(\"  • Decrease min_cluster_size (try 5-7)\")\n",
    "print(\"  • Decrease min_samples (try 1-2)\")\n",
    "print(\"  • Increase cluster_selection_epsilon (try 0.8-1.0)\")\n",
    "print(\"\\nIf you want TIGHTER, MORE DISTINCT clusters:\")\n",
    "print(\"  • Increase min_cluster_size (try 10-15)\")\n",
    "print(\"  • Increase min_samples (try 3-5)\")\n",
    "print(\"  • Decrease cluster_selection_epsilon (try 0.0-0.3)\")\n",
    "print(\"\\nIf clusters look too merged:\")\n",
    "print(\"  • Try different n_neighbors in UMAP (try 5-30)\")\n",
    "print(\"  • Adjust min_dist in UMAP (try 0.0-0.5)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
